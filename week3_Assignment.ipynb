{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1",
      "authorship_tag": "ABX9TyMMd8q/WZsm7fwE+i1ZOI1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pran9177r/info7375_Self_Improving_Ai/blob/main/week3_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> IMPORT REQUIRED LIB."
      ],
      "metadata": {
        "id": "Uw3TwNEPh4n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import math\n"
      ],
      "metadata": {
        "id": "wC8by6c5h3ov"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridworldFigure4_1:\n",
        "\n",
        "    def __init__(self, shape=(4, 4), gamma=0.9):\n",
        "        self.shape = shape\n",
        "        self.H, self.W = shape\n",
        "        self.states = [(r, c) for r in range(self.H) for c in range(self.W)]\n",
        "        self.terminal_states = [(0, 0), (self.H - 1, self.W - 1)]\n",
        "        self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "        self.action_symbols = ['↑', '↓', '←', '→']\n",
        "        self.num_actions = len(self.actions)\n",
        "        self.gamma = gamma\n",
        "        self.reward = -1.0\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        return state in self.terminal_states\n",
        "\n",
        "    def start_state(self):\n",
        "        non_terminals = [s for s in self.states if not self.is_terminal(s)]\n",
        "        return random.choice(non_terminals)\n",
        "\n",
        "    def _get_next_state(self, r, c, dr_a, dc_a):\n",
        "\n",
        "        next_r = max(0, min(r + dr_a, self.H - 1))\n",
        "        next_c = max(0, min(c + dc_a, self.W - 1))\n",
        "        return (next_r, next_c)\n",
        "\n",
        "    def step(self, state, action_idx):\n",
        "\n",
        "        if self.is_terminal(state): return state, 0.0\n",
        "\n",
        "        r, c = state\n",
        "        dr_a, dc_a = self.actions[action_idx]\n",
        "\n",
        "        intended_state = self._get_next_state(r, c, dr_a, dc_a)\n",
        "\n",
        "        if random.random() < 0.8:\n",
        "            next_state = intended_state\n",
        "        else: # 0.2 probability of staying put\n",
        "            next_state = state\n",
        "\n",
        "        return next_state, self.reward\n",
        "\n",
        "    def get_transition_prob(self, state, action_idx):\n",
        "\n",
        "        if self.is_terminal(state): return [(1.0, state, 0.0)]\n",
        "\n",
        "        r, c = state\n",
        "        dr_a, dc_a = self.actions[action_idx]\n",
        "\n",
        "        intended_state = self._get_next_state(r, c, dr_a, dc_a)\n",
        "\n",
        "        final_transitions = defaultdict(float)\n",
        "\n",
        "        final_transitions[intended_state] += 0.8\n",
        "\n",
        "        final_transitions[state] += 0.2\n",
        "\n",
        "        return [(p, s, self.reward) for s, p in final_transitions.items()]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DnIB7WCTh1JC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def greedy_policy(Q, state, env):\n",
        "    q_values = [Q[(state, a)] for a in range(env.num_actions)]\n",
        "    best_q = np.max(q_values)\n",
        "    return [a for a, q in enumerate(q_values) if abs(q - best_q) < 1e-6]\n",
        "\n",
        "def choose_action(Q, state, env, epsilon, policy_type='epsilon-greedy'):\n",
        "    if random.random() < epsilon and policy_type == 'epsilon-greedy':\n",
        "        return random.choice(range(env.num_actions))\n",
        "    else:\n",
        "        best_actions = greedy_policy(Q, state, env)\n",
        "        return random.choice(best_actions)\n",
        "\n",
        "def generate_episode(env, Q, epsilon):\n",
        "    episode = []; state = env.start_state()\n",
        "    while not env.is_terminal(state):\n",
        "        action_idx = choose_action(Q, state, env, epsilon, policy_type='epsilon-greedy')\n",
        "        next_state, reward = env.step(state, action_idx)\n",
        "        episode.append((state, action_idx, reward)); state = next_state\n",
        "    return episode\n",
        "\n",
        "\n",
        "def dp_control(env, theta=1e-6):\n",
        "    V = {s: 0.0 for s in env.states}\n",
        "    while True:\n",
        "        delta = 0; V_old = V.copy()\n",
        "        for s in env.states:\n",
        "            if env.is_terminal(s): continue\n",
        "            q_values = []\n",
        "            for a_idx in range(env.num_actions):\n",
        "                q_sa = sum(prob * (reward + env.gamma * V_old.get(next_s, 0.0))\n",
        "                           for prob, next_s, reward in env.get_transition_prob(s, a_idx))\n",
        "                q_values.append(q_sa)\n",
        "            V[s] = max(q_values); delta = max(delta, abs(V[s] - V_old[s]))\n",
        "        if delta < theta: break\n",
        "    pi = {};\n",
        "    for s in env.states:\n",
        "        if env.is_terminal(s): pi[s] = 'T'; continue\n",
        "        q_values = [sum(prob * (reward + env.gamma * V.get(next_s, 0.0))\n",
        "                        for prob, next_s, reward in env.get_transition_prob(s, a_idx))\n",
        "                    for a_idx in range(env.num_actions)]\n",
        "        best_actions = [env.action_symbols[a] for a, q in enumerate(q_values) if q == max(q_values)]\n",
        "        pi[s] = ' '.join(best_actions)\n",
        "    return V, pi\n",
        "\n",
        "\n",
        "def mc_control(env, num_episodes, epsilon, off_policy_type=None):\n",
        "    Q = defaultdict(float); C = defaultdict(float)\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env, Q, epsilon); G = 0; W = 1.0\n",
        "        for t in reversed(range(len(episode))):\n",
        "            S, A, R = episode[t]; G = R + env.gamma * G; s_a = (S, A)\n",
        "            if off_policy_type is None: # ON-POLICY\n",
        "                if s_a not in [(episode[i][0], episode[i][1]) for i in range(t)]:\n",
        "                    C[s_a] += 1; Q[s_a] += (G - Q[s_a]) / C[s_a]\n",
        "            else: # OFF-POLICY (IS)\n",
        "                best_actions = greedy_policy(Q, S, env); pi_prob = 1.0 / len(best_actions) if A in best_actions else 0.0\n",
        "                b_prob = epsilon / env.num_actions + (1 - epsilon) * (1.0 / env.num_actions if pi_prob == 0.0 else pi_prob)\n",
        "                if pi_prob == 0.0: break\n",
        "                C[s_a] += W\n",
        "                if off_policy_type == 'weighted': Q[s_a] += W / C[s_a] * (G - Q[s_a])\n",
        "                else: Q[s_a] += W * (G - Q[s_a]) / C[s_a]\n",
        "                W = W * (pi_prob / b_prob)\n",
        "    pi = {};\n",
        "    for s in env.states:\n",
        "        if env.is_terminal(s): pi[s] = 'T'; continue\n",
        "        best_actions = greedy_policy(Q, s, env); pi[s] = ' '.join(env.action_symbols[a] for a in best_actions)\n",
        "    return Q, pi\n",
        "\n",
        "\n",
        "def td_control(env, num_episodes, alpha, epsilon, control_type='sarsa'):\n",
        "    Q = defaultdict(float)\n",
        "    for _ in range(num_episodes):\n",
        "        S = env.start_state(); A = choose_action(Q, S, env, epsilon, policy_type='epsilon-greedy')\n",
        "        while not env.is_terminal(S):\n",
        "            S_prime, R = env.step(S, A)\n",
        "            if control_type == 'sarsa': # ON-POLICY\n",
        "                A_prime = choose_action(Q, S_prime, env, epsilon, policy_type='epsilon-greedy')\n",
        "                q_next = Q[(S_prime, A_prime)] if not env.is_terminal(S_prime) else 0.0\n",
        "                target = R + env.gamma * q_next\n",
        "            else: # OFF-POLICY (Q-Learning)\n",
        "                if env.is_terminal(S_prime): target = R\n",
        "                else: max_q_next = max([Q[(S_prime, a)] for a in range(env.num_actions)]); target = R + env.gamma * max_q_next\n",
        "                A_prime = choose_action(Q, S_prime, env, epsilon, policy_type='epsilon-greedy')\n",
        "            Q[(S, A)] += alpha * (target - Q[(S, A)]); S = S_prime; A = A_prime\n",
        "    pi = {};\n",
        "    for s in env.states:\n",
        "        if env.is_terminal(s): pi[s] = 'T'; continue\n",
        "        best_actions = greedy_policy(Q, s, env); pi[s] = ' '.join(env.action_symbols[a] for a in best_actions)\n",
        "    return Q, pi\n",
        "\n",
        "\n",
        "\n",
        "def visualize_policy(env, pi, title):\n",
        "    grid = np.full(env.shape, '', dtype=object)\n",
        "    for r in range(env.H):\n",
        "        for c in range(env.W):\n",
        "            state = (r, c)\n",
        "            if env.is_terminal(state): grid[r, c] = 'T'\n",
        "            else: grid[r, c] = pi.get(state, '?').split()[0]\n",
        "    print(f\"\\nPolicy: {title}\")\n",
        "    print(\"-\" * 25)\n",
        "    for row in grid: print(\"|\" + \"|\".join(f\"{item:^5s}\" for item in row) + \"|\")\n",
        "    print(\"-\" * 25)\n",
        "\n",
        "def value_iteration_step(env, V_k):\n",
        "\n",
        "    V_k_plus_1 = V_k.copy()\n",
        "    pi_k_plus_1 = {}\n",
        "    for s in env.states:\n",
        "        if env.is_terminal(s): pi_k_plus_1[s] = 'T'; continue\n",
        "        q_values = []\n",
        "        for a_idx in range(env.num_actions):\n",
        "            q_sa = sum(prob * (reward + env.gamma * V_k.get(next_s, 0.0))\n",
        "                       for prob, next_s, reward in env.get_transition_prob(s, a_idx))\n",
        "            q_values.append(q_sa)\n",
        "        V_k_plus_1[s] = max(q_values)\n",
        "        best_q = V_k_plus_1[s]\n",
        "        best_actions = [env.action_symbols[a] for a, q in enumerate(q_values) if abs(q - best_q) < 1e-6]\n",
        "        pi_k_plus_1[s] = ' '.join(best_actions)\n",
        "    return V_k_plus_1, pi_k_plus_1\n",
        "\n",
        "def print_figure_4_1_replication(env, steps_to_show):\n",
        "    V = {s: 0.0 for s in env.states}\n",
        "\n",
        "\n",
        "    def format_v(v_dict, k):\n",
        "        grid = np.full(env.shape, 0.0, dtype=float)\n",
        "        for (r, c), val in v_dict.items():\n",
        "            if not env.is_terminal((r, c)): grid[r, c] = val\n",
        "\n",
        "        print(f\"\\n$v_k$ for k={k}:\")\n",
        "\n",
        "        if k == 0:\n",
        "            print(np.around(grid, 0))\n",
        "        elif k == 1:\n",
        "            print(np.around(grid, 1))\n",
        "        elif k <= 3:\n",
        "\n",
        "            print(np.around(grid, 1))\n",
        "        else:\n",
        "\n",
        "            print(np.around(grid, 2))\n",
        "\n",
        "    def format_pi(pi_dict, k):\n",
        "        grid = np.full(env.shape, '', dtype=object)\n",
        "        for r in range(env.H):\n",
        "            for c in range(env.W):\n",
        "                state = (r, c)\n",
        "                if env.is_terminal(state): grid[r, c] = 'T'\n",
        "                else:\n",
        "                    actions = pi_dict.get(state, '↑ ↓ ← →').split()\n",
        "                    grid[r, c] = \"\".join(actions) if len(actions) <= 1 else 'All'\n",
        "\n",
        "        print(f\"Greedy Policy w.r.t. $v_k$ for k={k}:\")\n",
        "        print(grid)\n",
        "\n",
        "\n",
        "    V_k, pi_k = V.copy(), {s: '↑ ↓ ← →' for s in env.states if not env.is_terminal(s)}\n",
        "    pi_k[(0, 0)] = 'T'; pi_k[(3, 3)] = 'T'\n",
        "    format_v(V_k, 0); format_pi(pi_k, 0); print(\"-------------------------\")\n",
        "\n",
        "\n",
        "    for k in range(1, 11):\n",
        "        V_next, pi_next = value_iteration_step(env, V)\n",
        "        V = V_next\n",
        "\n",
        "        if k in steps_to_show or k == 10:\n",
        "            format_v(V, k)\n",
        "            format_pi(pi_next, k)\n",
        "            print(\"-------------------------\")"
      ],
      "metadata": {
        "id": "Vt6A7Nk7iOif"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> START MAIN CODE.\n"
      ],
      "metadata": {
        "id": "CmtfbZU-iQ6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    env = GridworldFigure4_1(gamma=0.9)\n",
        "\n",
        "\n",
        "    NUM_EPISODES = 100000\n",
        "    ALPHA = 0.1\n",
        "    EPSILON = 0.1\n",
        "\n",
        "    print(\"--- 1. DP CONTROL: Figure 4.1 Replication (Value Iteration) ---\")\n",
        "\n",
        "\n",
        "    print_figure_4_1_replication(env, steps_to_show=[1, 2, 3])\n",
        "\n",
        "    print(\"\\n\\n--- 2. RL CONTROL ALGORITHM RESULTS (Policy Visualizations) ---\")\n",
        "\n",
        "\n",
        "    V_dp_final, pi_dp_final = dp_control(env, theta=1e-6)\n",
        "    visualize_policy(env, pi_dp_final, \"1. DP Control (Optimal Policy)\")\n",
        "\n",
        "\n",
        "    _, pi_mc_on = mc_control(env, NUM_EPISODES, EPSILON, off_policy_type=None)\n",
        "    visualize_policy(env, pi_mc_on, \"2. MC On-Policy Control\")\n",
        "\n",
        "\n",
        "    _, pi_mc_off_unw = mc_control(env, NUM_EPISODES, EPSILON, off_policy_type='unweighted')\n",
        "    visualize_policy(env, pi_mc_off_unw, \"3. MC Off-Policy (Unweighted IS)\")\n",
        "\n",
        "\n",
        "    _, pi_mc_off_w = mc_control(env, NUM_EPISODES, EPSILON, off_policy_type='weighted')\n",
        "    visualize_policy(env, pi_mc_off_w, \"4. MC Off-Policy (Weighted IS)\")\n",
        "\n",
        "\n",
        "    _, pi_sarsa = td_control(env, NUM_EPISODES, ALPHA, EPSILON, control_type='sarsa')\n",
        "    visualize_policy(env, pi_sarsa, \"5. TD(0) On-Policy Control (SARSA)\")\n",
        "\n",
        "\n",
        "    _, pi_qlearn = td_control(env, NUM_EPISODES, ALPHA, EPSILON, control_type='q_learning')\n",
        "    visualize_policy(env, pi_qlearn, \"6. TD(0) Off-Policy Control (Q-Learning)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZChiNvMRful0",
        "outputId": "0db04232-fb21-4c02-a55c-ed77ad7c0dbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. DP CONTROL: Figure 4.1 Replication (Value Iteration) ---\n",
            "\n",
            "$v_k$ for k=0:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "Greedy Policy w.r.t. $v_k$ for k=0:\n",
            "[['T' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'T']]\n",
            "-------------------------\n",
            "\n",
            "$v_k$ for k=1:\n",
            "[[ 0. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1. -1.]\n",
            " [-1. -1. -1.  0.]]\n",
            "Greedy Policy w.r.t. $v_k$ for k=1:\n",
            "[['T' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' 'T']]\n",
            "-------------------------\n",
            "\n",
            "$v_k$ for k=2:\n",
            "[[ 0.  -1.2 -1.9 -1.9]\n",
            " [-1.2 -1.9 -1.9 -1.9]\n",
            " [-1.9 -1.9 -1.9 -1.2]\n",
            " [-1.9 -1.9 -1.2  0. ]]\n",
            "Greedy Policy w.r.t. $v_k$ for k=2:\n",
            "[['T' '←' 'All' 'All']\n",
            " ['↑' 'All' 'All' 'All']\n",
            " ['All' 'All' 'All' '↓']\n",
            " ['All' 'All' '→' 'T']]\n",
            "-------------------------\n",
            "\n",
            "$v_k$ for k=3:\n",
            "[[ 0.  -1.2 -2.2 -2.7]\n",
            " [-1.2 -2.2 -2.7 -2.2]\n",
            " [-2.2 -2.7 -2.2 -1.2]\n",
            " [-2.7 -2.2 -1.2  0. ]]\n",
            "Greedy Policy w.r.t. $v_k$ for k=3:\n",
            "[['T' '←' '←' 'All']\n",
            " ['↑' 'All' 'All' '↓']\n",
            " ['↑' 'All' 'All' '↓']\n",
            " ['All' '→' '→' 'T']]\n",
            "-------------------------\n",
            "\n",
            "$v_k$ for k=10:\n",
            "[[ 0.   -1.22 -2.29 -3.23]\n",
            " [-1.22 -2.29 -3.23 -2.29]\n",
            " [-2.29 -3.23 -2.29 -1.22]\n",
            " [-3.23 -2.29 -1.22  0.  ]]\n",
            "Greedy Policy w.r.t. $v_k$ for k=10:\n",
            "[['T' '←' '←' 'All']\n",
            " ['↑' 'All' 'All' '↓']\n",
            " ['↑' 'All' 'All' '↓']\n",
            " ['All' '→' '→' 'T']]\n",
            "-------------------------\n",
            "\n",
            "\n",
            "--- 2. RL CONTROL ALGORITHM RESULTS (Policy Visualizations) ---\n",
            "\n",
            "Policy: 1. DP Control (Optimal Policy)\n",
            "-------------------------\n",
            "|  T  |  ←  |  ←  |  ↓  |\n",
            "|  ↑  |  ↑  |  ↑  |  ↓  |\n",
            "|  ↑  |  ↑  |  ↓  |  ↓  |\n",
            "|  ↑  |  →  |  →  |  T  |\n",
            "-------------------------\n",
            "\n",
            "Policy: 2. MC On-Policy Control\n",
            "-------------------------\n",
            "|  T  |  ←  |  ←  |  ↓  |\n",
            "|  ↑  |  ←  |  ←  |  ↓  |\n",
            "|  ↑  |  →  |  →  |  ↓  |\n",
            "|  ↑  |  →  |  →  |  T  |\n",
            "-------------------------\n",
            "\n",
            "Policy: 3. MC Off-Policy (Unweighted IS)\n",
            "-------------------------\n",
            "|  T  |  ↑  |  ↑  |  ↑  |\n",
            "|  ↑  |  ↑  |  ↑  |  ↑  |\n",
            "|  ↓  |  ↑  |  ↑  |  ↑  |\n",
            "|  ↑  |  ←  |  ↓  |  T  |\n",
            "-------------------------\n",
            "\n",
            "Policy: 4. MC Off-Policy (Weighted IS)\n",
            "-------------------------\n",
            "|  T  |  ↑  |  ↑  |  ↑  |\n",
            "|  ↓  |  ↑  |  ↑  |  ↑  |\n",
            "|  ↑  |  ←  |  ↑  |  ↑  |\n",
            "|  ↑  |  ↑  |  ↓  |  T  |\n",
            "-------------------------\n",
            "\n",
            "Policy: 5. TD(0) On-Policy Control (SARSA)\n",
            "-------------------------\n",
            "|  T  |  ←  |  ←  |  ↓  |\n",
            "|  ↑  |  ←  |  ↓  |  ↓  |\n",
            "|  ↑  |  →  |  ↓  |  ↓  |\n",
            "|  ↑  |  →  |  →  |  T  |\n",
            "-------------------------\n",
            "\n",
            "Policy: 6. TD(0) Off-Policy Control (Q-Learning)\n",
            "-------------------------\n",
            "|  T  |  ←  |  ←  |  ↓  |\n",
            "|  ↑  |  ↑  |  →  |  ↓  |\n",
            "|  ↑  |  ↑  |  ↓  |  ↓  |\n",
            "|  →  |  →  |  →  |  T  |\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}